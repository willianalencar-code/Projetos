import streamlit as st
import duckdb
from huggingface_hub import hf_hub_download
import pandas as pd
import os
import tempfile
from datetime import datetime

# ==========================================
# CONFIGURA√á√ÉO
# ==========================================
st.set_page_config(
    page_title="Exportador de Clientes 7M",
    layout="wide",
    page_icon="üöÄ"
)

@st.cache_data(show_spinner=False)
def get_dataset():
    """Baixa o arquivo do HF"""
    try:
        token = st.secrets["HF_TOKEN"]
        caminho_local = hf_hub_download(
            repo_id="WillianAlencar/SegmentacaoClientes",
            filename="data/train-00000-of-00001.parquet",
            repo_type="dataset",
            token=token
        )
        return caminho_local
    except Exception as e:
        st.error(f"Erro: {e}")
        return None

@st.cache_resource(show_spinner=False)
def get_connection():
    return duckdb.connect(database=':memory:')

# ==========================================
# FUN√á√ïES DE EXPORTA√á√ÉO OTIMIZADAS
# ==========================================
def export_chunked(con, query, chunk_size=500000, format="csv"):
    """Exporta dados em peda√ßos"""
    total_query = f"SELECT COUNT(*) FROM ({query})"
    total_count = con.execute(total_query).fetchone()[0]
    
    num_chunks = (total_count // chunk_size) + (1 if total_count % chunk_size > 0 else 0)
    
    results = []
    for i in range(num_chunks):
        offset = i * chunk_size
        chunk_query = f"SELECT * FROM ({query}) LIMIT {chunk_size} OFFSET {offset}"
        df_chunk = con.execute(chunk_query).df()
        results.append(df_chunk)
        
        # Progresso
        progress = (i + 1) / num_chunks
        st.progress(progress, text=f"Processando peda√ßo {i+1}/{num_chunks}")
    
    if results:
        return pd.concat(results, ignore_index=True)
    return pd.DataFrame()

# ==========================================
# INTERFACE PRINCIPAL
# ==========================================
caminho_arquivo = get_dataset()
con = get_connection()

if caminho_arquivo:
    con.execute(f"CREATE OR REPLACE TABLE clientes AS SELECT * FROM read_parquet('{caminho_arquivo}')")
    
    st.title("üöÄ Exportador de Dados - 7 Milh√µes de Clientes")
    
    # Sidebar
    st.sidebar.header("‚öôÔ∏è Configura√ß√µes de Exporta√ß√£o")
    
    # Op√ß√µes principais
    export_mode = st.sidebar.radio(
        "Modo de exporta√ß√£o:",
        ["üí° Amostra R√°pida", "üìä Dados Filtrados", "üöÄ Dataset Completo (7M)"],
        help="Amostra: 100K linhas | Filtrados: Aplicando filtros | Completo: Todos os 7M"
    )
    
    # Filtros (apenas para modo filtrado)
    cat_sel, setor_sel = [], []
    if export_mode == "üìä Dados Filtrados":
        st.sidebar.subheader("üîç Filtros")
        
        categorias = con.execute("SELECT DISTINCT categoria FROM clientes LIMIT 50").df()['categoria'].tolist()
        cat_sel = st.sidebar.multiselect("Categorias:", categorias)
        
        setores = con.execute("SELECT DISTINCT setor FROM clientes LIMIT 50").df()['setor'].tolist()
        setor_sel = st.sidebar.multiselect("Setores:", setores)
    
    # Filtro de datas
    st.sidebar.subheader("üîç Filtro de Data")
    min_date, max_date = con.execute("SELECT MIN(created_at), MAX(created_at) FROM clientes").fetchone()
    if min_date and max_date:
        date_range = st.sidebar.date_input(
            "Per√≠odo",
            value=[min_date, max_date],
            min_value=min_date,
            max_value=max_date
        )
    else:
        date_range = None
    
    # Exportar apenas member_pk
    only_member_pk = st.sidebar.checkbox("Exportar apenas member_pk", value=False, help="Somente a coluna de identifica√ß√£o")
    
    # Formato
    export_format = st.sidebar.selectbox(
        "Formato:",
        ["Parquet (Recomendado)", "CSV", "JSON"],
        help="Parquet: Mais r√°pido e menor | CSV: Universal | JSON: Para APIs"
    )
    
    # Op√ß√µes avan√ßadas
    with st.sidebar.expander("‚ö° Op√ß√µes Avan√ßadas"):
        use_chunks = st.checkbox("Dividir em partes", value=True, help="Divide a exporta√ß√£o para evitar travamentos")
        if use_chunks:
            chunk_size = st.select_slider(
                "Registros por parte:",
                options=[100000, 250000, 500000, 1000000],
                value=500000
            )
        compress = st.checkbox("Comprimir arquivo", value=True, help="Reduz tamanho do arquivo (especialmente √∫til para CSV/JSON)")
    
    # Informa√ß√µes gerais
    total_clientes = con.execute("SELECT COUNT(*) FROM clientes").fetchone()[0]
    st.sidebar.info(f"**Total na base:** {total_clientes:,} clientes")
    
    # ==========================================
    # PR√â-VISUALIZA√á√ÉO
    # ==========================================
    st.header("üìã Pr√©-visualiza√ß√£o")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Clientes Totais", f"{total_clientes:,}")
    with col2:
        st.metric("Modo Selecionado", export_mode.split()[0])
    with col3:
        st.metric("Formato", export_format.split()[0])
    
    st.subheader("üëÅÔ∏è Amostra dos Dados (100 primeiros)")
    sample_df = con.execute("SELECT * FROM clientes LIMIT 100").df()
    st.dataframe(sample_df, use_container_width=True)
    
    # ==========================================
    # PREPARAR QUERY DE EXPORTA√á√ÉO
    # ==========================================
    if export_mode == "üí° Amostra R√°pida":
        base_query = "SELECT * FROM clientes LIMIT 100000"
        estimated_rows = 100000
    elif export_mode == "üìä Dados Filtrados":
        base_query = "SELECT * FROM clientes WHERE 1=1"
        if cat_sel:
            cat_values = ", ".join([f"'{c}'" for c in cat_sel])
            base_query += f" AND categoria IN ({cat_values})"
        if setor_sel:
            setor_values = ", ".join([f"'{s}'" for s in setor_sel])
            base_query += f" AND setor IN ({setor_values})"
        if date_range:
            start_date = date_range[0].strftime('%Y-%m-%d')
            end_date = date_range[1].strftime('%Y-%m-%d')
            base_query += f" AND created_at BETWEEN '{start_date}' AND '{end_date}'"
        try:
            estimated_rows = con.execute(f"SELECT COUNT(*) FROM ({base_query})").fetchone()[0]
        except:
            estimated_rows = con.execute("SELECT COUNT(*) FROM clientes").fetchone()[0]
    else:  # Dataset Completo
        base_query = "SELECT * FROM clientes"
        if date_range:
            start_date = date_range[0].strftime('%Y-%m-%d')
            end_date = date_range[1].strftime('%Y-%m-%d')
            base_query += f" AND created_at BETWEEN '{start_date}' AND '{end_date}'"
        estimated_rows = total_clientes
    
    # Aplicar somente member_pk
    if only_member_pk:
        base_query = base_query.replace("SELECT *", "SELECT member_pk")
    
    # Contagem de member_pk √∫nico
    unique_members_filtered = con.execute(f"SELECT COUNT(DISTINCT member_pk) FROM ({base_query})").fetchone()[0]
    st.info(f"Clientes √∫nicos na exporta√ß√£o: **{unique_members_filtered:,}**")
    
    # Nome do arquivo
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    file_ext_preview = export_format.split()[0].lower()
    file_name_preview = f"clientes_{export_mode.split()[0].lower()}_{timestamp}.{file_ext_preview}"
    st.info(f"Nome do arquivo que ser√° gerado: **{file_name_preview}**")
    
    st.info(f"**üìä Estimativa:** {estimated_rows:,} registros ser√£o exportados")
    
    # ==========================================
    # BOT√ÉO DE EXPORTA√á√ÉO
    # ==========================================
    if st.button("üöÄ INICIAR EXPORTA√á√ÉO", type="primary", use_container_width=True):
        with st.spinner(f"Preparando exporta√ß√£o de {estimated_rows:,} registros..."):
            try:
                # Criar arquivo tempor√°rio
                with tempfile.NamedTemporaryFile(delete=False, suffix=f".{export_format.split()[0].lower()}") as tmp_file:
                    tmp_path = tmp_file.name
                
                # Exportar usando DuckDB direto
                if export_format == "Parquet (Recomendado)":
                    con.execute(f"COPY ({base_query}) TO '{tmp_path}' (FORMAT PARQUET)")
                    mime_type = "application/octet-stream"
                    file_ext = "parquet"
                elif export_format == "CSV":
                    if compress:
                        con.execute(f"COPY ({base_query}) TO '{tmp_path}' (FORMAT CSV, HEADER true, COMPRESSION GZIP)")
                        file_ext = "csv.gz"
                        mime_type = "application/gzip"
                    else:
                        con.execute(f"COPY ({base_query}) TO '{tmp_path}' (FORMAT CSV, HEADER true)")
                        file_ext = "csv"
                        mime_type = "text/csv"
                elif export_format == "JSON":
                    if use_chunks and estimated_rows > chunk_size:
                        df_export = export_chunked(con, base_query, chunk_size)
                    else:
                        df_export = con.execute(base_query).df()
                    if compress:
                        df_export.to_json(tmp_path, orient='records', lines=True, compression='gzip')
                        file_ext = "json.gz"
                        mime_type = "application/gzip"
                    else:
                        df_export.to_json(tmp_path, orient='records', lines=True)
                        file_ext = "json"
                        mime_type = "application/json"
                
                # Ler arquivo gerado
                file_size = os.path.getsize(tmp_path) / (1024 * 1024)  # MB
                with open(tmp_path, 'rb') as f:
                    file_data = f.read()
                
                os.unlink(tmp_path)
                
                st.success(f"‚úÖ Exporta√ß√£o conclu√≠da! Arquivo: {file_size:.2f} MB")
                
                # Bot√£o de download
                filename = f"clientes_{export_mode.split()[0].lower()}_{timestamp}.{file_ext}"
                st.download_button(
                    label=f"üì• BAIXAR ARQUIVO ({file_size:.2f} MB)",
                    data=file_data,
                    file_name=filename,
                    mime=mime_type,
                    use_container_width=True,
                    type="primary"
                )
                
                # Estat√≠sticas
                with st.expander("üìä Estat√≠sticas da Exporta√ß√£o"):
                    col_stat1, col_stat2, col_stat3 = st.columns(3)
                    with col_stat1:
                        st.metric("Registros Exportados", f"{estimated_rows:,}")
                    with col_stat2:
                        st.metric("Tamanho do Arquivo", f"{file_size:.2f} MB")
                    with col_stat3:
                        compression_ratio = (estimated_rows * 100) / (file_size * 1024 * 1024) if file_size > 0 else 0
                        st.metric("Taxa Compress√£o", f"{compression_ratio:.1f} bytes/registro")
                
            except Exception as e:
                st.error(f"‚ùå Erro durante exporta√ß√£o: {str(e)}")
                with st.expander("üõ†Ô∏è Solu√ß√µes poss√≠veis"):
                    st.markdown("""
                    **Se a exporta√ß√£o falhou:**
                    1. **Tente exportar em partes menores** - Use a op√ß√£o "Dividir em partes"
                    2. **Use formato Parquet** - √â mais eficiente que CSV
                    3. **Exporte apenas uma amostra** - 100K registros primeiro
                    4. **Verifique sua conex√£o** - 7M registros exigem boa conex√£o
                    5. **Tente novamente em alguns minutos** - Pode ser congestionamento tempor√°rio
                    """)
    
    # ==========================================
    # DICAS
    # ==========================================
    with st.expander("üí° Dicas para Exporta√ß√£o de Grandes Volumes"):
        st.markdown("""
        **Para 7 milh√µes de registros:**
        
        ü•á **Parquet √© o MELHOR formato:**
        - 10x mais r√°pido que CSV
        - 5x menor em tamanho
        - Mant√©m tipos de dados
        
        ‚ö° **Performance:**
        - Exporta√ß√£o completa: 2-5 minutos
        - Tamanho estimado: 200-500 MB (Parquet)
        - Tamanho CSV: 1-2 GB
        
        üõ°Ô∏è **Seguran√ßa:**
        - Dados processados em mem√≥ria
        - Arquivo tempor√°rio √© apagado
        - Nenhum dado fica no servidor
        
        üì± **Como usar depois:**
        ```python
        # Para Parquet:
        import pandas as pd
        df = pd.read_parquet('arquivo.parquet')
        
        # Para CSV comprimido:
        df = pd.read_csv('arquivo.csv.gz')
        ```
        """)
else:
    st.warning("Configure o token HF_TOKEN nos secrets do Streamlit Cloud")
