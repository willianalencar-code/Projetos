import streamlit as st
import duckdb
from huggingface_hub import hf_hub_download
import pandas as pd
import os
import tempfile
from datetime import datetime

# ==========================================
# CONFIGURA√á√ÉO
# ==========================================
st.set_page_config(
    page_title="Exportador de Clientes 7M",
    layout="wide",
    page_icon="üöÄ"
)

@st.cache_data(show_spinner=False)
def get_dataset():
    """Baixa o arquivo do HF"""
    try:
        token = st.secrets["HF_TOKEN"]
        caminho_local = hf_hub_download(
            repo_id="WillianAlencar/SegmentacaoClientes",
            filename="data/train-00000-of-00001.parquet",
            repo_type="dataset",
            token=token
        )
        return caminho_local
    except Exception as e:
        st.error(f"Erro: {e}")
        return None

@st.cache_resource(show_spinner=False)
def get_connection():
    return duckdb.connect(database=':memory:')

# ==========================================
# FUN√á√ïES DE EXPORTA√á√ÉO OTIMIZADAS
# ==========================================
def export_chunked(con, query, chunk_size=500000):
    """Exporta dados em peda√ßos"""
    total_query = f"SELECT COUNT(*) FROM ({query})"
    total_count = con.execute(total_query).fetchone()[0]
    num_chunks = (total_count // chunk_size) + (1 if total_count % chunk_size > 0 else 0)
    results = []

    for i in range(num_chunks):
        offset = i * chunk_size
        chunk_query = f"SELECT * FROM ({query}) LIMIT {chunk_size} OFFSET {offset}"
        df_chunk = con.execute(chunk_query).df()
        results.append(df_chunk)
        progress = (i + 1) / num_chunks
        st.progress(progress, text=f"Processando peda√ßo {i+1}/{num_chunks}")

    if results:
        return pd.concat(results, ignore_index=True)
    return pd.DataFrame()

# ==========================================
# INTERFACE PRINCIPAL
# ==========================================
caminho_arquivo = get_dataset()
con = get_connection()

if caminho_arquivo:
    con.execute(f"CREATE OR REPLACE TABLE clientes AS SELECT * FROM read_parquet('{caminho_arquivo}')")
    st.title("üöÄ Exportador de Dados - 7 Milh√µes de Clientes")

    # Sidebar
    st.sidebar.header("‚öôÔ∏è Configura√ß√µes de Exporta√ß√£o")

    # Export mode
    export_mode = st.sidebar.radio(
        "Modo de exporta√ß√£o:",
        ["üí° Amostra R√°pida", "üìä Dados Filtrados", "üöÄ Dataset Completo (7M)"],
        help="Amostra: 100K linhas | Filtrados: Aplicando filtros | Completo: Todos os 7M"
    )

    # ==========================================
    # FILTROS DE CATEGORIA E SETOR
    # ==========================================
    if "cat_sel" not in st.session_state:
        st.session_state["cat_sel"] = []
    if "setor_sel" not in st.session_state:
        st.session_state["setor_sel"] = []

    categorias = con.execute("SELECT DISTINCT categoria FROM clientes LIMIT 50").df()['categoria'].tolist()
    setores = con.execute("SELECT DISTINCT setor FROM clientes LIMIT 50").df()['setor'].tolist()

    cat_sel = st.sidebar.multiselect(
        "Categorias:",
        options=categorias,
        default=st.session_state["cat_sel"]
    )
    st.session_state["cat_sel"] = cat_sel

    setor_sel = st.sidebar.multiselect(
        "Setores:",
        options=setores,
        default=st.session_state["setor_sel"]
    )
    st.session_state["setor_sel"] = setor_sel

    # ==========================================
    # FILTROS DE DATAS
    # ==========================================
    st.sidebar.subheader("üîç Filtro de Datas")
    min_visita, max_visita = con.execute("SELECT MIN(data_ultima_visita), MAX(data_ultima_visita) FROM clientes").fetchone()
    min_compra, max_compra = con.execute("SELECT MIN(data_ultima_compra), MAX(data_ultima_compra) FROM clientes").fetchone()

    if "date_visita" not in st.session_state:
        st.session_state["date_visita"] = [min_visita, max_visita]
    if "date_compra" not in st.session_state:
        st.session_state["date_compra"] = [min_compra, max_compra]

    date_visita = st.sidebar.date_input(
        "Per√≠odo da √öltima Visita",
        value=st.session_state["date_visita"],
        min_value=min_visita,
        max_value=max_visita
    )
    st.session_state["date_visita"] = date_visita

    date_compra = st.sidebar.date_input(
        "Per√≠odo da √öltima Compra",
        value=st.session_state["date_compra"],
        min_value=min_compra,
        max_value=max_compra
    )
    st.session_state["date_compra"] = date_compra

    # ==========================================
    # OP√á√ÉO SOMENTE MEMBER_PK
    # ==========================================
    only_member_pk = st.sidebar.checkbox(
        "Exportar apenas member_pk",
        value=False,
        help="Somente a coluna de identifica√ß√£o"
    )

    # ==========================================
    # FORMATO E OP√á√ïES AVAN√áADAS
    # ==========================================
    export_format = st.sidebar.selectbox(
        "Formato:",
        ["Parquet (Recomendado)", "CSV", "JSON"],
        help="Parquet: Mais r√°pido e menor | CSV: Universal | JSON: Para APIs"
    )

    with st.sidebar.expander("‚ö° Op√ß√µes Avan√ßadas"):
        use_chunks = st.checkbox("Dividir em partes", value=True, help="Divide a exporta√ß√£o para evitar travamentos")
        if use_chunks:
            chunk_size = st.select_slider(
                "Registros por parte:",
                options=[100000, 250000, 500000, 1000000],
                value=500000
            )
        compress = st.checkbox("Comprimir arquivo", value=True, help="Reduz tamanho do arquivo (especialmente √∫til para CSV/JSON)")

    # ==========================================
    # INFORMA√á√ïES GERAIS
    # ==========================================
    total_clientes = con.execute("SELECT COUNT(*) FROM clientes").fetchone()[0]
    st.sidebar.info(f"**Total na base:** {total_clientes:,} clientes")

    # ==========================================
    # PR√â-VISUALIZA√á√ÉO
    # ==========================================
    st.header("üìã Pr√©-visualiza√ß√£o")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Clientes Totais", f"{total_clientes:,}")
    with col2:
        st.metric("Modo Selecionado", export_mode.split()[0])
    with col3:
        st.metric("Formato", export_format.split()[0])

    st.subheader("üëÅÔ∏è Amostra dos Dados (100 primeiros)")
    sample_df = con.execute("SELECT * FROM clientes LIMIT 100").df()
    st.dataframe(sample_df, use_container_width=True)

    # ==========================================
    # PREPARAR QUERY
    # ==========================================
    if export_mode == "üí° Amostra R√°pida":
        base_query = "SELECT * FROM clientes LIMIT 100000"
        estimated_rows = 100000
    else:
        base_query = "SELECT * FROM clientes WHERE 1=1"
        # filtros de categoria/setor
        if cat_sel:
            cat_values = ", ".join([f"'{c}'" for c in cat_sel])
            base_query += f" AND categoria IN ({cat_values})"
        if setor_sel:
            setor_values = ", ".join([f"'{s}'" for s in setor_sel])
            base_query += f" AND setor IN ({setor_values})"
        # filtros de datas
        if date_visita:
            start_visita = date_visita[0].strftime('%Y-%m-%d')
            end_visita = date_visita[1].strftime('%Y-%m-%d')
            base_query += f" AND data_ultima_visita BETWEEN '{start_visita}' AND '{end_visita}'"
        if date_compra:
            start_compra = date_compra[0].strftime('%Y-%m-%d')
            end_compra = date_compra[1].strftime('%Y-%m-%d')
            base_query += f" AND data_ultima_compra BETWEEN '{start_compra}' AND '{end_compra}'"

        # estimativa
        try:
            estimated_rows = con.execute(f"SELECT COUNT(*) FROM ({base_query})").fetchone()[0]
        except:
            estimated_rows = total_clientes

    # aplicar somente member_pk
    if only_member_pk:
        base_query = base_query.replace("SELECT *", "SELECT member_pk")

    # quantidade de member_pk √∫nico
    unique_members_filtered = con.execute(f"SELECT COUNT(DISTINCT member_pk) FROM ({base_query})").fetchone()[0]
    st.info(f"Clientes √∫nicos na exporta√ß√£o: **{unique_members_filtered:,}**")

    # nome do arquivo
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    file_ext_preview = export_format.split()[0].lower()
    file_name_preview = f"clientes_{export_mode.split()[0].lower()}_{timestamp}.{file_ext_preview}"
    st.info(f"Nome do arquivo que ser√° gerado: **{file_name_preview}**")
    st.info(f"**üìä Estimativa:** {estimated_rows:,} registros ser√£o exportados")

    # ==========================================
    # BOT√ÉO DE EXPORTA√á√ÉO
    # ==========================================
    if st.button("üöÄ INICIAR EXPORTA√á√ÉO", type="primary", use_container_width=True):
        with st.spinner(f"Preparando exporta√ß√£o de {estimated_rows:,} registros..."):
            try:
                with tempfile.NamedTemporaryFile(delete=False, suffix=f".{export_format.split()[0].lower()}") as tmp_file:
                    tmp_path = tmp_file.name

                # exporta√ß√£o
                if export_format == "Parquet (Recomendado)":
                    con.execute(f"COPY ({base_query}) TO '{tmp_path}' (FORMAT PARQUET)")
                    mime_type = "application/octet-stream"
                    file_ext = "parquet"
                elif export_format == "CSV":
                    if compress:
                        con.execute(f"COPY ({base_query}) TO '{tmp_path}' (FORMAT CSV, HEADER true, COMPRESSION GZIP)")
                        file_ext = "csv.gz"
                        mime_type = "application/gzip"
                    else:
                        con.execute(f"COPY ({base_query}) TO '{tmp_path}' (FORMAT CSV, HEADER true)")
                        file_ext = "csv"
                        mime_type = "text/csv"
                elif export_format == "JSON":
                    if use_chunks and estimated_rows > chunk_size:
                        df_export = export_chunked(con, base_query, chunk_size)
                    else:
                        df_export = con.execute(base_query).df()
                    if compress:
                        df_export.to_json(tmp_path, orient='records', lines=True, compression='gzip')
                        file_ext = "json.gz"
                        mime_type = "application/gzip"
                    else:
                        df_export.to_json(tmp_path, orient='records', lines=True)
                        file_ext = "json"
                        mime_type = "application/json"

                # ler arquivo
                file_size = os.path.getsize(tmp_path) / (1024 * 1024)
                with open(tmp_path, 'rb') as f:
                    file_data = f.read()
                os.unlink(tmp_path)

                st.success(f"‚úÖ Exporta√ß√£o conclu√≠da! Arquivo: {file_size:.2f} MB")

                # bot√£o de download
                filename = f"clientes_{export_mode.split()[0].lower()}_{timestamp}.{file_ext}"
                st.download_button(
                    label=f"üì• BAIXAR ARQUIVO ({file_size:.2f} MB)",
                    data=file_data,
                    file_name=filename,
                    mime=mime_type,
                    use_container_width=True,
                    type="primary"
                )

                # estat√≠sticas
                with st.expander("üìä Estat√≠sticas da Exporta√ß√£o"):
                    col_stat1, col_stat2, col_stat3 = st.columns(3)
                    with col_stat1:
                        st.metric("Registros Exportados", f"{estimated_rows:,}")
                    with col_stat2:
                        st.metric("Tamanho do Arquivo", f"{file_size:.2f} MB")
                    with col_stat3:
                        compression_ratio = (estimated_rows * 100) / (file_size * 1024 * 1024) if file_size > 0 else 0
                        st.metric("Taxa Compress√£o", f"{compression_ratio:.1f} bytes/registro")

            except Exception as e:
                st.error(f"‚ùå Erro durante exporta√ß√£o: {str(e)}")
                with st.expander("üõ†Ô∏è Solu√ß√µes poss√≠veis"):
                    st.markdown("""
                    **Se a exporta√ß√£o falhou:**
                    1. **Tente exportar em partes menores** - Use a op√ß√£o "Dividir em partes"
                    2. **Use formato Parquet** - √â mais eficiente que CSV
                    3. **Exporte apenas uma amostra** - 100K registros primeiro
                    4. **Verifique sua conex√£o** - 7M registros exigem boa conex√£o
                    5. **Tente novamente em alguns minutos** - Pode ser congestionamento tempor√°rio
                    """)

    # ==========================================
    # DICAS
    # ==========================================
    with st.expander("üí° Dicas para Exporta√ß√£o de Grandes Volumes"):
        st.markdown("""
        **Para 7 milh√µes de registros:**
        
        ü•á **Parquet √© o MELHOR formato:**
        - 10x mais r√°pido que CSV
        - 5x menor em tamanho
        - Mant√©m tipos de dados
        
        ‚ö° **Performance:**
        - Exporta√ß√£o completa: 2-5 minutos
        - Tamanho estimado: 200-500 MB (Parquet)
        - Tamanho CSV: 1-2 GB
        
        üõ°Ô∏è **Seguran√ßa:**
        - Dados processados em mem√≥ria
        - Arquivo tempor√°rio √© apagado
        - Nenhum dado fica no servidor
        
        üì± **Como usar depois:**
        ```python
        # Para Parquet:
        import pandas as pd
        df = pd.read_parquet('arquivo.parquet')
        
        # Para CSV comprimido:
        df = pd.read_csv('arquivo.csv.gz')
        ```
        """)
else:
    st.warning("Configure o token HF_TOKEN nos secrets do Streamlit Cloud")
